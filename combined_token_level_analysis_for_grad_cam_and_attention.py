# -*- coding: utf-8 -*-
"""Combined Token level analysis for Grad-CAM and Attention.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SFZWtLDVKzeZFG2E52BZR6cZxZfQk3Se
"""

# ===================================================================
# Cell 1: Setup & Mount Drive
# ===================================================================
from google.colab import drive
drive.mount('/content/drive')
from google.colab import files
import os

print("ðŸ“¤ Upload the 3 pre-split JSON files from train_test_0 folder:")
print("   - ocr_train.json")
print("   - ocr_val.json")
print("   - ocr_test.json")

uploaded = files.upload()

# Verify uploads
print("\nâœ… Upload verification:")
for filename in ['ocr_train.json', 'ocr_val.json', 'ocr_test.json']:
    if filename in uploaded:
        print(f"   âœ… {filename} ({len(uploaded[filename])} bytes)")
    else:
        raise FileNotFoundError(f"   âŒ Missing: {filename} - Please upload it!")

print("\nâœ… All files uploaded successfully!")

# ===================================================================
# Cell 2: Install Dependencies & Main Imports
# ===================================================================
!pip install -q transformers datasets accelerate pillow editdistance jiwer opencv-python matplotlib seaborn scipy pandas

import os
import json
import random
from pathlib import Path
from datetime import datetime
import numpy as np
import pandas as pd
import torch
import cv2
from PIL import Image
import matplotlib.pyplot as plt
import seaborn as sns
import time
from scipy.stats import pearsonr, spearmanr, linregress

# Configuration
DRIVE_BASE = "/content/drive/MyDrive"
IMG_DIR = Path(f"{DRIVE_BASE}/OMR/images/Dataset cortona")
TRAIN_JSON = Path("/content/ocr_train.json")
VAL_JSON = Path("/content/ocr_val.json")
TEST_JSON = Path("/content/ocr_test.json")
PROCESSED_DIR = Path("/content/processed_data")
OUTPUT_BASE = Path("/content/trocr_ablation_results")

PROCESSED_DIR.mkdir(exist_ok=True)
OUTPUT_BASE.mkdir(exist_ok=True)

# Experiment selection
EXPERIMENT = "freeze_none"  # Options: baseline, no_clahe, no_aug, freeze_none

# Training hyperparameters
NUM_EPOCHS = 50
TRAIN_BATCH = 4
EVAL_BATCH = 8
GRAD_ACCUM = 4
LR = 3e-5

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# ===================================================================
# Cell 3: Load Pre-Split Data from train_test_2 folder
# ===================================================================

print("ðŸ“‚ Loading pre-split data from train_test_2 folder...")

# Check files exist
for json_path in [TRAIN_JSON, VAL_JSON, TEST_JSON]:
    if not json_path.exists():
        raise FileNotFoundError(f"âŒ Missing file: {json_path}")

# Load the splits
with open(TRAIN_JSON, "r", encoding="utf-8") as f:
    train_data = json.load(f)

with open(VAL_JSON, "r", encoding="utf-8") as f:
    val_data = json.load(f)

with open(TEST_JSON, "r", encoding="utf-8") as f:
    test_data = json.load(f)

# Extract annotations
train_anns = train_data["annotations"]
val_anns = val_data["annotations"]
test_anns = test_data["annotations"]

# Create unified image map
all_images = train_data.get("images", []) + val_data.get("images", []) + test_data.get("images", [])
image_map = {img["id"]: img for img in all_images}

# Remove duplicates in image_map (in case images appear in multiple files)
unique_images = {}
for img in all_images:
    if img["id"] not in unique_images:
        unique_images[img["id"]] = img
image_map = unique_images

print(f"âœ… Loaded pre-split data:")
print(f"   Train: {len(train_anns)} annotations")
print(f"   Val:   {len(val_anns)} annotations")
print(f"   Test:  {len(test_anns)} annotations")
print(f"   Total unique images: {len(image_map)}")

# Filter for text-line category (category_id = 6)
TEXT_LINE_CATEGORY = 6

train_anns = [ann for ann in train_anns
              if ann.get("category_id") == TEXT_LINE_CATEGORY and
              ann.get("description", "").strip()]
val_anns = [ann for ann in val_anns
            if ann.get("category_id") == TEXT_LINE_CATEGORY and
            ann.get("description", "").strip()]
test_anns = [ann for ann in test_anns
             if ann.get("category_id") == TEXT_LINE_CATEGORY and
             ann.get("description", "").strip()]

print(f"\nâœ… After filtering for text-line category:")
print(f"   Train: {len(train_anns)} text lines")
print(f"   Val:   {len(val_anns)} text lines")
print(f"   Test:  {len(test_anns)} text lines")

# Verification: Check for data leakage
train_ids = set([ann["id"] for ann in train_anns])
val_ids = set([ann["id"] for ann in val_anns])
test_ids = set([ann["id"] for ann in test_anns])

overlaps = {
    "train-val": len(train_ids & val_ids),
    "train-test": len(train_ids & test_ids),
    "val-test": len(val_ids & test_ids)
}

print(f"\nâœ… Data leakage check:")
print(f"   Train-Val overlap: {overlaps['train-val']} (should be 0)")
print(f"   Train-Test overlap: {overlaps['train-test']} (should be 0)")
print(f"   Val-Test overlap: {overlaps['val-test']} (should be 0)")

if sum(overlaps.values()) == 0:
    print("   âœ… PASSED: No data leakage detected")
else:
    print("   âš ï¸ WARNING: Data leakage detected!")

# Store filtered data
filtered = {
    "info": train_data.get("info", {}),
    "licenses": train_data.get("licenses", []),
    "categories": train_data.get("categories", []),
    "images": list(image_map.values()),
    "annotations": train_anns + val_anns + test_anns
}

# ===================================================================
# Cell 4: Data Augmentation Class
# ===================================================================
class ManuscriptAugmentation:
    def __call__(self, image: Image.Image) -> Image.Image:
        img_np = np.array(image)

        # Rotation
        if random.random() < 0.4:
            angle = random.uniform(-5, 5)
            h, w = img_np.shape[:2]
            M = cv2.getRotationMatrix2D((w/2, h/2), angle, 1.0)
            img_np = cv2.warpAffine(img_np, M, (w, h), borderMode=cv2.BORDER_REPLICATE)

        # Elastic deformation
        if random.random() < 0.25:
            h, w = img_np.shape[:2]
            if h > 20 and w > 20:
                dx = cv2.resize(np.random.randn(max(h//10, 3), max(w//10, 3)), (w, h)) * 3
                dy = cv2.resize(np.random.randn(max(h//10, 3), max(w//10, 3)), (w, h)) * 3
                x, y = np.meshgrid(np.arange(w), np.arange(h))
                map_x = np.clip(x + dx, 0, w-1).astype(np.float32)
                map_y = np.clip(y + dy, 0, h-1).astype(np.float32)
                img_np = cv2.remap(img_np, map_x, map_y, cv2.INTER_LINEAR)

        # Gaussian blur
        if random.random() < 0.35:
            kernel_size = random.choice([3,5,7])
            sigma = random.uniform(0.5,1.2)
            img_np = cv2.GaussianBlur(img_np, (kernel_size,kernel_size), sigma)

        # Brightness
        if random.random() < 0.4:
            factor = random.uniform(0.7,1.3)
            img_np = np.clip(img_np * factor, 0, 255).astype(np.uint8)

        # Contrast
        if random.random() < 0.3:
            factor = random.uniform(0.8,1.2)
            mean = img_np.mean()
            img_np = np.clip((img_np-mean)*factor + mean, 0, 255).astype(np.uint8)

        # Speckle noise
        if random.random() < 0.25:
            noise = np.random.randn(*img_np.shape) * 6
            img_np = np.clip(img_np + noise, 0, 255).astype(np.uint8)

        # Morphological operations
        if random.random() < 0.15:
            kernel = np.ones((2,2), np.uint8)
            if random.random() < 0.5:
                img_np = cv2.erode(img_np, kernel, iterations=1)
            else:
                img_np = cv2.dilate(img_np, kernel, iterations=1)

        # Shadow/staining
        if random.random() < 0.15:
            h, w = img_np.shape[:2]
            if h > 10 and w > 10:
                shadow = np.random.randn(max(h//5,2), max(w//5,2))
                shadow = cv2.resize(shadow, (w,h))
                shadow = (shadow - shadow.min()) / (shadow.max()-shadow.min()+1e-8)
                shadow = 1 - shadow * 0.25
                if len(img_np.shape) == 3:
                    shadow = np.expand_dims(shadow, axis=-1)
                img_np = np.clip(img_np * shadow, 0, 255).astype(np.uint8)

        return Image.fromarray(img_np)

# ===================================================================
# Cell 5: Dataset Class
# ===================================================================
from torch.utils.data import Dataset
from transformers import TrOCRProcessor

class TrOCRDataset(Dataset):
    def __init__(self, annotations, images_info, image_root, processor: TrOCRProcessor,
                 max_target_length=128, augment_transform=None, use_clahe=True):
        self.annotations = annotations
        self.image_root = Path(image_root)
        self.processor = processor
        self.max_target_length = max_target_length
        self.augment_transform = augment_transform
        self.use_clahe = use_clahe
        self.image_id_to_info = images_info
        print(f"Dataset: {len(self.annotations)} samples, CLAHE={self.use_clahe}, AUG={augment_transform is not None}")

    def preprocess_image(self, img_np):
        if len(img_np.shape) == 3:
            gray = cv2.cvtColor(img_np, cv2.COLOR_BGR2GRAY)
        else:
            gray = img_np

        if self.use_clahe:
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
            enhanced = clahe.apply(gray)
        else:
            enhanced = cv2.equalizeHist(gray)

        rgb = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2RGB)
        return rgb

    def __len__(self):
        return len(self.annotations)

    def __getitem__(self, idx):
        ann = self.annotations[idx]
        text = ann.get("description", "") or ann.get("text", "") or " "
        img_info = self.image_id_to_info[ann["image_id"]]
        image_path = self.image_root / img_info["file_name"]

        img = cv2.imread(str(image_path))
        if img is None:
            raise FileNotFoundError(f"Image not found: {image_path}")

        x,y,w,h = [int(v) for v in ann["bbox"]]
        margin = 8
        x = max(0, x-margin)
        y = max(0, y-margin)
        w = min(img.shape[1] - x, w + 2*margin)
        h = min(img.shape[0] - y, h + 2*margin)

        crop = img[y:y+h, x:x+w]
        crop = self.preprocess_image(crop)
        pil = Image.fromarray(crop)

        if self.augment_transform:
            pil = self.augment_transform(pil)

        encoding = self.processor(
            images=pil,
            text=text,
            padding="max_length",
            max_length=self.max_target_length,
            truncation=True,
            return_tensors="pt"
        )

        encoding = {k: v.squeeze(0) for k, v in encoding.items()}
        return encoding

# ===================================================================
# Cell 6: Combined Token level analysis for Grad-CAM and Attention
# ===================================================================

import torch
import torch.nn.functional as F
import cv2
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import pandas as pd
from scipy.stats import pearsonr, spearmanr
import json
from tqdm import tqdm
import editdistance

# ============================================================================
# GRAD-CAM: Token-Level Analysis
# ============================================================================

class TokenLevelTrOCRGradCAM:
    """Token-by-token Grad-CAM for TrOCR"""
    def __init__(self, model, device):
        self.model = model
        self.device = device
        self.gradients = None
        self.activations = None

        target_layer = self.model.encoder.encoder.layer[-1]
        target_layer.register_forward_hook(self._save_activation)
        target_layer.register_full_backward_hook(self._save_gradient)

    def _save_activation(self, module, input, output):
        self.activations = output.detach()

    def _save_gradient(self, module, grad_input, grad_output):
        self.gradients = grad_output[0].detach()

    def generate_token_level_cams(self, pixel_values, target_ids):
        """Generate separate Grad-CAM for EACH token"""
        self.model.eval()
        pixel_values = pixel_values.to(self.device)
        target_ids = target_ids.to(self.device)

        pixel_values.requires_grad = True
        outputs = self.model(
            pixel_values=pixel_values,
            decoder_input_ids=target_ids,
            return_dict=True
        )

        logits = outputs.logits
        valid_mask = (target_ids[0] != -100) & (target_ids[0] != self.model.config.pad_token_id)
        valid_positions = torch.where(valid_mask)[0]

        token_cams = []

        for pos in valid_positions:
            token_id = target_ids[0, pos].item()
            token_logit = logits[0, pos, token_id]

            self.model.zero_grad()
            token_logit.backward(retain_graph=True)

            if self.gradients is not None and self.activations is not None:
                gradients = self.gradients[0]
                activations = self.activations[0]
                weights = gradients.mean(dim=0)

                cam = torch.zeros(activations.size(0), device=self.device)
                for i in range(activations.size(1)):
                    cam += weights[i] * activations[:, i]

                cam = F.relu(cam)
                if cam.max() > 0:
                    cam = cam / cam.max()

                cam = cam[1:]
                grid_size = int(np.sqrt(len(cam)))
                cam_2d = cam[:grid_size*grid_size].reshape(grid_size, grid_size)

                loss = F.cross_entropy(
                    logits[0, pos:pos+1],
                    target_ids[0, pos:pos+1],
                    reduction='none'
                ).item()

                token_cams.append({
                    'position': pos.item(),
                    'token_id': token_id,
                    'cam': cam_2d.cpu().numpy(),
                    'loss': loss
                })

        return token_cams


# ============================================================================
# ATTENTION: Token-Level Analysis
# ============================================================================

class TokenLevelAttentionExtractor:
    """
    Extract cross-attention maps for each token
    Shows which image patches the decoder attended to to for each prediction
    """
    def __init__(self, model, device):
        # Simplified init: no need to register hooks manually, outputs.cross_attentions provides them.
        self.model = model
        self.device = device

    def extract_token_level_attention(self, pixel_values, target_ids):
        """
        Extract attention maps for each token separately

        Returns:
            token_attentions: List of attention maps per token
        """
        self.model.eval()

        pixel_values = pixel_values.to(self.device)
        target_ids = target_ids.to(self.device)

        with torch.no_grad():
            outputs = self.model(
                pixel_values=pixel_values,
                decoder_input_ids=target_ids,
                output_attentions=True, # Ensure attention outputs are generated
                return_dict=True
            )

        # Get valid token positions
        valid_mask = (target_ids[0] != -100) & (target_ids[0] != self.model.config.pad_token_id)
        valid_positions = torch.where(valid_mask)[0]

        # Extract cross-attention from the LAST decoder layer
        # outputs.cross_attentions is a tuple of (batch, num_heads, tgt_len, src_len) tensors
        # one for each decoder layer's cross-attention
        if outputs.cross_attentions and len(outputs.cross_attentions) > 0:
            # Shape: (batch, num_heads, tgt_len, src_len)
            last_layer_attention = outputs.cross_attentions[-1]

            token_attentions = []

            for pos in valid_positions:
                token_id = target_ids[0, pos].item()

                # Get attention for this specific token position
                # Average across attention heads
                token_attn = last_layer_attention[0, :, pos, :].mean(dim=0)  # (src_len,)

                # Remove CLS token and reshape to 2D
                # The vision encoder output includes a CLS token at the beginning
                token_attn = token_attn[1:]  # Remove CLS token
                grid_size = int(np.sqrt(len(token_attn))) # Assuming square patches for simplicity
                attn_2d = token_attn[:grid_size*grid_size].reshape(grid_size, grid_size)

                # Normalize to [0, 1]
                attn_2d = attn_2d / (attn_2d.max() + 1e-10)

                # Get logits for loss calculation
                loss = F.cross_entropy(
                    outputs.logits[0, pos:pos+1], # logits for this token
                    target_ids[0, pos:pos+1],    # actual target token
                    reduction='none'
                ).item()

                token_attentions.append({
                    'position': pos.item(),
                    'token_id': token_id,
                    'attention': attn_2d.cpu().numpy(),
                    'loss': loss
                })

            return token_attentions

        return []


# ============================================================================
# METRICS & ANALYSIS
# ============================================================================

def compute_map_metrics(map_2d):
    """Compute metrics for attention/CAM map"""
    map_flat = map_2d.flatten()

    # Gini coefficient
    sorted_map = np.sort(map_flat)
    n = len(sorted_map)
    cumsum = np.cumsum(sorted_map)
    if cumsum[-1] > 0:
        gini = (2 * np.sum((np.arange(1, n+1)) * sorted_map)) / (n * cumsum[-1]) - (n+1)/n
    else:
        gini = 0.0

    peak = map_2d.max()
    coverage = (map_2d > 0.5).sum() / map_2d.size

    map_norm = map_flat / (map_flat.sum() + 1e-10)
    entropy = -np.sum(map_norm * np.log(map_norm + 1e-10))
    max_entropy = np.log(len(map_flat))
    normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0

    h, w = map_2d.shape
    center_mask = np.zeros_like(map_2d)
    center_h, center_w = h//4, w//4
    center_mask[center_h:3*center_h, center_w:3*center_w] = 1
    center_bias = (map_2d * center_mask).sum() / (map_2d.sum() + 1e-10)

    return {
        'gini_coefficient': gini,
        'peak_activation': peak,
        'coverage': coverage,
        'entropy': normalized_entropy,
        'center_bias': center_bias
    }


def analyze_sample_combined(model, dataset_item, processor, device, save_dir=None):
    """
    Combined Grad-CAM + Attention analysis for one sample
    """
    # Grad-CAM analysis
    gradcam = TokenLevelTrOCRGradCAM(model, device)
    attention_extractor = TokenLevelAttentionExtractor(model, device)

    pixel = dataset_item["pixel_values"].unsqueeze(0)
    labels = dataset_item["labels"].unsqueeze(0)

    decoder_input = labels.new_zeros(labels.shape)
    decoder_input[:, 1:] = labels[:, :-1].clone()
    decoder_input[:, 0] = processor.tokenizer.cls_token_id

    # Get both Grad-CAM and Attention
    token_cams = gradcam.generate_token_level_cams(pixel, decoder_input)
    token_attentions = attention_extractor.extract_token_level_attention(pixel, decoder_input)

    # Get ground truth and prediction
    label_str = processor.batch_decode(labels, skip_special_tokens=True)[0]

    model.eval()
    with torch.no_grad():
        generated = model.generate(pixel.to(device), max_length=128, num_beams=4)
        pred_str = processor.batch_decode(generated, skip_special_tokens=True)[0]

    cer = editdistance.eval(pred_str, label_str) / max(len(label_str), 1)

    # Analyze each token for BOTH methods
    combined_results = []
    for cam_data, attn_data in zip(token_cams, token_attentions):
        cam_metrics = compute_map_metrics(cam_data['cam'])
        attn_metrics = compute_map_metrics(attn_data['attention'])

        combined = {
            'position': cam_data['position'],
            'token_id': cam_data['token_id'],
            'token_char': processor.tokenizer.decode([cam_data['token_id']]),
            'token_loss': cam_data['loss'],

            # Grad-CAM metrics
            'gradcam_gini': cam_metrics['gini_coefficient'],
            'gradcam_peak': cam_metrics['peak_activation'],
            'gradcam_coverage': cam_metrics['coverage'],
            'gradcam_entropy': cam_metrics['entropy'],

            # Attention metrics
            'attention_gini': attn_metrics['gini_coefficient'],
            'attention_peak': attn_metrics['peak_activation'],
            'attention_coverage': attn_metrics['coverage'],
            'attention_entropy': attn_metrics['entropy'],
        }
        combined_results.append(combined)

    # Visualize if requested
    if save_dir:
        visualize_combined_analysis(
            pixel[0], token_cams, token_attentions, combined_results,
            pred_str, label_str, cer, processor, save_dir
        )

    return {
        'image_cer': cer,
        'prediction': pred_str,
        'ground_truth': label_str,
        'token_results': combined_results,
        'mean_gradcam_gini': np.mean([t['gradcam_gini'] for t in combined_results]),
        'mean_attention_gini': np.mean([t['attention_gini'] for t in combined_results]),
        'mean_loss': np.mean([t['token_loss'] for t in combined_results])
    }


def visualize_combined_analysis(image_tensor, token_cams, token_attentions,
                                 token_results, prediction, ground_truth,
                                 cer, processor, save_dir):
    """
    Visualize BOTH Grad-CAM and Attention side-by-side for each token
    """
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)

    img = image_tensor.cpu().permute(1, 2, 0).numpy()
    img = ((img - img.min()) / (img.max() - img.min()) * 255).astype(np.uint8)
    h, w = img.shape[:2]

    num_tokens = len(token_cams)

    # Create figure with comparison layout
    fig = plt.figure(figsize=(20, num_tokens * 2 + 4))
    gs = fig.add_gridspec(num_tokens + 2, 4, hspace=0.4, wspace=0.3)

    # Header
    ax_img = fig.add_subplot(gs[0, :2])
    ax_img.imshow(img, cmap='gray')
    ax_img.set_title('Original Image', fontsize=14, fontweight='bold')
    ax_img.axis('off')

    ax_summary = fig.add_subplot(gs[0, 2:])
    summary_text = f"""
Ground Truth: "{ground_truth}"
Prediction:   "{prediction}"

Overall CER: {cer:.4f}
Tokens: {num_tokens}

Mean Grad-CAM Gini: {np.mean([t['gradcam_gini'] for t in token_results]):.3f}
Mean Attention Gini: {np.mean([t['attention_gini'] for t in token_results]):.3f}
Mean Loss: {np.mean([t['token_loss'] for t in token_results]):.4f}
"""
    ax_summary.text(0.05, 0.95, summary_text, transform=ax_summary.transAxes,
                    fontsize=11, verticalalignment='top', fontfamily='monospace',
                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))
    ax_summary.axis('off')

    # Token-by-token: Grad-CAM vs Attention
    for idx, (cam_data, attn_data, metrics) in enumerate(zip(token_cams, token_attentions, token_results)):
        row = 1 + idx

        # Original crop
        ax_orig = fig.add_subplot(gs[row, 0])
        ax_orig.imshow(img, cmap='gray')
        ax_orig.axis('off')

        # Grad-CAM
        ax_cam = fig.add_subplot(gs[row, 1])
        cam_resized = cv2.resize(cam_data['cam'].astype(np.float32), (w, h), interpolation=cv2.INTER_CUBIC)
        heatmap_cam = cv2.applyColorMap((cam_resized * 255).astype(np.uint8), cv2.COLORMAP_JET)
        overlay_cam = cv2.addWeighted(img, 0.6, heatmap_cam, 0.4, 0)
        ax_cam.imshow(overlay_cam)
        ax_cam.set_title(f"Grad-CAM\nGini: {metrics['gradcam_gini']:.3f}", fontsize=10)
        ax_cam.axis('off')

        # Attention
        ax_attn = fig.add_subplot(gs[row, 2])
        attn_resized = cv2.resize(attn_data['attention'].astype(np.float32), (w, h), interpolation=cv2.INTER_CUBIC)
        heatmap_attn = cv2.applyColorMap((attn_resized * 255).astype(np.uint8), cv2.COLORMAP_JET)
        overlay_attn = cv2.addWeighted(img, 0.6, heatmap_attn, 0.4, 0)
        ax_attn.imshow(overlay_attn)
        ax_attn.set_title(f"Attention\nGini: {metrics['attention_gini']:.3f}", fontsize=10)
        ax_attn.axis('off')

        # Metrics
        ax_metrics = fig.add_subplot(gs[row, 3])
        char = metrics['token_char']
        pos = metrics['position']
        loss = metrics['token_loss']

        metrics_text = f"""Token: '{char}' (pos {pos})
Loss: {loss:.3f}

Grad-CAM:
  Gini: {metrics['gradcam_gini']:.3f}
  Coverage: {metrics['gradcam_coverage']:.1%}
  Entropy: {metrics['gradcam_entropy']:.3f}

Attention:
  Gini: {metrics['attention_gini']:.3f}
  Coverage: {metrics['attention_coverage']:.1%}
  Entropy: {metrics['attention_entropy']:.3f}
"""
        ax_metrics.text(0.05, 0.95, metrics_text, transform=ax_metrics.transAxes,
                       fontsize=8, verticalalignment='top', fontfamily='monospace',
                       bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))
        ax_metrics.axis('off')

    # Comparison plot
    ax_compare = fig.add_subplot(gs[-1, :])
    positions = [t['position'] for t in token_results]
    gradcam_ginis = [t['gradcam_gini'] for t in token_results]
    attention_ginis = [t['attention_gini'] for t in token_results]
    losses = [t['token_loss'] for t in token_results]

    ax_loss = ax_compare.twinx()
    ax_compare.plot(positions, gradcam_ginis, 'b-o', label='Grad-CAM Gini', linewidth=2)
    ax_compare.plot(positions, attention_ginis, 'g-s', label='Attention Gini', linewidth=2)
    ax_loss.plot(positions, losses, 'r-^', label='Token Loss', linewidth=2, alpha=0.7)

    ax_compare.set_xlabel('Token Position', fontsize=11)
    ax_compare.set_ylabel('Gini Coefficient', fontsize=11, color='black')
    ax_loss.set_ylabel('Loss', fontsize=11, color='r')
    ax_loss.tick_params(axis='y', labelcolor='r')
    ax_compare.grid(alpha=0.3)
    ax_compare.legend(loc='upper left')
    ax_loss.legend(loc='upper right')
    ax_compare.set_title('Grad-CAM vs Attention: Token-Level Comparison', fontsize=12, fontweight='bold')

    plt.tight_layout()
    plt.savefig(save_dir / 'combined_analysis.png', dpi=150, bbox_inches='tight')
    plt.close()


def run_combined_analysis(model, test_dataset, processor, device, output_dir, num_samples=None):
    """
    Run complete Grad-CAM + Attention analysis
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    if num_samples is None:
        num_samples = len(test_dataset)

    all_token_results = []
    image_results = []

    print(f"\n{'='*70}")
    print("COMBINED GRAD-CAM + ATTENTION ANALYSIS (Token-Level)")
    print(f"{'='*70}\n")

    for idx in tqdm(range(min(num_samples, len(test_dataset))), desc="Analyzing samples"):
        try:
            item = test_dataset[idx]
            save_dir = output_dir / "combined_visualizations" / f"sample_{idx:04d}"

            result = analyze_sample_combined(model, item, processor, device, save_dir)

            image_results.append({
                'sample_id': idx,
                'cer': result['image_cer'],
                'prediction': result['prediction'],
                'ground_truth': result['ground_truth'],
                'num_tokens': len(result['token_results']),
                'mean_gradcam_gini': np.mean([t['gradcam_gini'] for t in result['token_results']]),
                'mean_attention_gini': np.mean([t['attention_gini'] for t in result['token_results']]),
                'mean_loss': np.mean([t['token_loss'] for t in result['token_results']])
            })

            for token_res in result['token_results']:
                token_res['sample_id'] = idx
                token_res['image_cer'] = result['image_cer']
                all_token_results.append(token_res)

        except Exception as e:
            print(f"\nError processing sample {idx}: {e}")
            continue

    token_df = pd.DataFrame(all_token_results)
    image_df = pd.DataFrame(image_results)

    token_df.to_csv(output_dir / "combined_token_results.csv", index=False)
    image_df.to_csv(output_dir / "combined_image_results.csv", index=False)

    perform_combined_statistical_analysis(token_df, image_df, output_dir)

    print(f"\nâœ… Combined analysis complete! Results saved to {output_dir}")

    return token_df, image_df


def perform_combined_statistical_analysis(token_df, image_df, output_dir):
    """Compare Grad-CAM vs Attention"""
    print("\n" + "="*70)
    print("COMPARATIVE ANALYSIS: GRAD-CAM vs ATTENTION")
    print("="*70)

    # Check if token_df is empty to avoid KeyError
    if token_df.empty:
        print("No token-level data to analyze. Skipping statistical analysis.")
        return

    # Correlation with token loss
    print("\n1. Correlation with Token Loss")
    print("-" * 50)

    for method in ['gradcam', 'attention']:
        gini_col = f'{method}_gini'
        r, p = pearsonr(token_df['token_loss'], token_df[gini_col])
        rho, p_spear = spearmanr(token_df['token_loss'], token_df[gini_col])

        print(f"\n{method.upper()}:")
        print(f"  Pearson:  r = {r:7.4f}, p = {p:.4e}")
        print(f"  Spearman: Ï = {rho:7.4f}, p = {p_spear:.4e}")

    # Agreement between methods
    print("\n2. Agreement: Grad-CAM vs Attention")
    print("-" * 50)

    r_agree, p_agree = pearsonr(token_df['gradcam_gini'], token_df['attention_gini'])
    print(f"Correlation between methods: r = {r_agree:.4f}, p = {p_agree:.4e}")

    # Which is more predictive?
    print("\n3. Predictive Power Comparison")
    print("-" * 50)

    from sklearn.metrics import r2_score
    from scipy.stats import linregress

    _, _, r_grad, _, _ = linregress(token_df['gradcam_gini'], token_df['token_loss'])
    _, _, r_attn, _, _ = linregress(token_df['attention_gini'], token_df['token_loss'])

    print(f"Grad-CAM RÂ² with loss: {r_grad**2:.4f}")
    print(f"Attention RÂ² with loss: {r_attn**2:.4f}")

    if r_grad**2 > r_attn**2:
        print(f"\nâœ… Grad-CAM is MORE predictive (Î” = {(r_grad**2 - r_attn**2):.4f})")
    else:
        print(f"\nâœ… Attention is MORE predictive (Î” = {(r_attn**2 - r_grad**2):.4f})")

    create_comparison_plots(token_df, image_df, output_dir)


def create_comparison_plots(token_df, image_df, output_dir):
    """Create comparison plots"""
    plots_dir = output_dir / "plots"
    plots_dir.mkdir(parents=True, exist_ok=True)

    # 1. Direct comparison
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))

    axes[0].scatter(token_df['gradcam_gini'], token_df['token_loss'],
                    alpha=0.5, s=20, label='Grad-CAM')
    axes[0].set_xlabel('Grad-CAM Gini', fontsize=12)
    axes[0].set_ylabel('Token Loss', fontsize=12)
    axes[0].set_title('Grad-CAM: Localization vs Loss', fontsize=13, fontweight='bold')
    axes[0].grid(alpha=0.3)

    axes[1].scatter(token_df['attention_gini'], token_df['token_loss'],
                    alpha=0.5, s=20, color='green', label='Attention')
    axes[1].set_xlabel('Attention Gini', fontsize=12)
    axes[1].set_ylabel('Token Loss', fontsize=12)
    axes[1].set_title('Attention: Localization vs Loss', fontsize=13, fontweight='bold')
    axes[1].grid(alpha=0.3)

    plt.tight_layout()
    plt.savefig(plots_dir / 'gradcam_vs_attention_comparison.png', dpi=150)
    plt.close()

    # 2. Agreement plot
    plt.figure(figsize=(8, 8))
    plt.scatter(token_df['gradcam_gini'], token_df['attention_gini'],
                alpha=0.5, s=20, c=token_df['token_loss'], cmap='coolwarm')
    plt.colorbar(label='Token Loss')
    plt.plot([0, 1], [0, 1], 'r--', alpha=0.5, label='Perfect agreement')
    plt.xlabel('Grad-CAM Gini', fontsize=12)
    plt.ylabel('Attention Gini', fontsize=12)
    plt.title('Method Agreement', fontsize=14, fontweight='bold')
    plt.legend()
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig(plots_dir / 'method_agreement.png', dpi=150)
    plt.close()

    print(f"\nâœ… Comparison plots saved to {plots_dir}")

# ===================================================================
# Cell 7: Main running: Token level analysis for Grad-CAM and Attention (Combined)
# ===================================================================

# Load model and processor
from transformers import VisionEncoderDecoderModel, TrOCRProcessor

model_path = "/content/drive/MyDrive/freezenoneupdatedepochpatience_results" #Path of your trained model
model = VisionEncoderDecoderModel.from_pretrained(model_path)
processor = TrOCRProcessor.from_pretrained(model_path)
model.to(device)

# Re-create test dataset (already exists in your notebook)
test_dataset = TrOCRDataset(
    test_anns, image_map, IMG_DIR, processor,
    max_target_length=128,
    augment_transform=None,
    use_clahe=True
)
# Run combined analysis
output_dir = Path("/content/combined_gradcam_attention_analysis")

token_df, image_df = run_combined_analysis(
    model=model,
    test_dataset=test_dataset,
    processor=processor,
    device=device,
    output_dir=output_dir,
    num_samples=120 # Adjust as needed
)

print("\n" + "="*70)
print("KEY FINDINGS:")
print("="*70)
print(f"Total tokens analyzed: {len(token_df)}")
print(f"Grad-CAM Gini: {token_df['gradcam_gini'].mean():.4f}")
print(f"Attention Gini: {token_df['attention_gini'].mean():.4f}")
print(f"\nCorrelations with Token Loss:")
print(f"  Grad-CAM: {token_df[['token_loss', 'gradcam_gini']].corr().iloc[0,1]:.4f}")
print(f"  Attention: {token_df[['token_loss', 'attention_gini']].corr().iloc[0,1]:.4f}")